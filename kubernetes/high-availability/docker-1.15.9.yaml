---
- hosts: all
  tasks:
  - name: Mapping new hosts
    lineinfile:
      path: /etc/hosts
      line: |
        # k8s-api
        10.15.0.10 k8s-api.beruanglaut.cloud

        # master Node
        10.15.0.11 jh-k8s-master001 master001
        10.15.0.12 jh-k8s-master002 master002
        10.15.0.13 jh-k8s-master003 master003

        # worker Node
        10.15.0.14 jh-k8s-worker001 worker001
        10.15.0.15 jh-k8s-worker002 worker002
        10.15.0.16 jh-k8s-worker003 worker003
      insertafter: EOF
  - name: Install depedency
    apt:
      name: "{{ packages }}"
      update_cache: yes
      state: present
    vars:
      packages:
      - apt-transport-https
      - ca-certificates
      - curl
      - gnupg
      - lsb-release
      - nfs-common
  - name: Load Docker required modules
    copy:
      dest: /etc/modules-load.d/docker.conf
      content: |
        overlay
        br_netfilter
  - name: Load required sysctl parameters
    copy:
      dest: /etc/sysctl.d/99-kubernetes-cri.conf
      content: |
        net.bridge.bridge-nf-call-iptables  = 1
        net.bridge.bridge-nf-call-ip6tables = 1
        net.ipv4.ip_forward                 = 1
  - name: Applying modules and parameters
    shell: |
      modprobe overlay
      modprobe br_netfilter
      sysctl --system
  - name: Install Docker binaries
    apt:
      name: "{{ packages }}"
      state: present
      update_cache: yes
    vars:
      packages:
        - docker.io=20.10.7-0ubuntu5~20.04.2
        - containerd
  - name: Create Docker directories
    file:
      path: /etc/docker
      state: directory
  - name: Configuring Docker
    copy:
      dest: /etc/docker/daemon.json
      content: |
        {
          "exec-opts": ["native.cgroupdriver=systemd"],
          "log-driver": "json-file",
          "log-opts": {
          "max-size": "100m"
        },
          "storage-driver": "overlay2"
        }
  - name: Enable Docker service
    systemd:
      name: docker
      enabled: yes
  - name: Reloading Docker service
    systemd:
      name: docker
      daemon_reload: yes
  - name: Restarting Docker service
    systemd:
      name: docker
      state: restarted
  - name: Remove swapfile from /etc/fstab
    mount:
      name: "{{ item }}"
      fstype: swap
      state: absent
    with_items:
      - swap
      - none
  - name: Disable swap
    command: swapoff -a
    when: ansible_swaptotal_mb > 0
  - name: Add an apt signing key for Kubernetes
    apt_key:
      url: https://packages.cloud.google.com/apt/doc/apt-key.gpg
      state: present
  - name: Adding apt repository for Kubernetes
    apt_repository:
      repo: deb https://apt.kubernetes.io/ kubernetes-xenial main
      state: present
      filename: kubernetes.list
  - name: Install Kubernetes binaries
    apt:
      name: "{{ packages }}"
      state: present
      update_cache: yes
    vars:
      packages:
        - kubelet=1.15.9-00
        - kubeadm=1.15.9-00
        - kubectl=1.15.9-00
  - name: Disable auto updates for Kubernetes packages
    dpkg_selections:
      name: "{{ item }}"
      selection: hold
    loop:
      - kubelet
      - kubeadm
      - kubectl
      - docker.io
      - containerd
- hosts: masters
  tasks:
  - name: Install haproxy & keepalived
    apt:
      name: "{{ packages }}"
      update_cache: yes
      state: present
    vars:
      packages:
      - haproxy
      - keepalived
  - name: Create keepalived_script user
    shell: |
      groupadd -r keepalived_script
      useradd -r -s /sbin/nologin -g keepalived_script -M keepalived_script
- hosts: master001
  tasks:
  - name: Create Keepalived configuration for master001
    copy:
      src: keepalived/master001.conf
      dest: /etc/keepalived/keepalived.conf
- hosts: master002
  tasks:
  - name: Create Keepalived configuration for master002
    copy:
      src: keepalived/master002.conf
      dest: /etc/keepalived/keepalived.conf
- hosts: master003
  tasks:
  - name: Create Keepalived configuration for master002
    copy:
      src: keepalived/master003.conf
      dest: /etc/keepalived/keepalived.conf
- hosts: masters
  tasks:
  - name: Create Keepalived script for checking Kubernetes API
    copy:
      src: keepalived/check_apiserver.sh
      dest: /etc/keepalived/check_apiserver.sh
      mode: 0755
  - name: Configuring Haproxy
    copy:
      src: keepalived/haproxy.cfg
      dest: /etc/haproxy/haproxy.cfg
  - name: Restarting Haproxy & Keepalived service
    systemd:
      name: "{{ item }}"
      state: restarted
    loop:
      - haproxy
      - keepalived
- hosts: master001
  tasks:
  - name: Creating kubeadm config file
    copy:
      src: kubeadm-config/kubernetes-1.15.9.yaml
      dest: /root/kubeadm-config.yaml
  - name: Initialize the first Kubernetes master
    shell: kubeadm init --config /root/kubeadm-config.yaml
  - name: Configure kubelet to use haproxy frontend port
    shell: sed -i 's/6443/8443/g' /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf
  - name: Apply kubelet haproxy configuration
    systemd:
      name: kubelet
      state: restarted
  - name: Apply Calico pod network
    shell: |
      mkdir -p /root/.kube
      cp -i /etc/kubernetes/admin.conf /root/.kube/config
      chown $(id -u):$(id -g) /root/.kube/config
      kubectl apply -f https://docs.projectcalico.org/v3.14/manifests/calico.yaml
  - name: Configure kubernetes cluster to use haproxy frontend port
    shell: |
      kubectl -n kube-system get configmap kubeadm-config -o jsonpath='{.data.ClusterConfiguration}' > /root/kubeadm-config.yaml
      sed -i 's/6443/8443/g' /root/kubeadm-config.yaml
      kubeadm init phase upload-config kubeadm --config /root/kubeadm-config.yaml
      kubectl -n kube-system get cm kube-proxy -o yaml > /root/kube-proxy.yaml
      sed -i 's/6443/8443/g' /root/kube-proxy.yaml
      kubectl apply -f /root/kube-proxy.yaml
      kubectl -n kube-system rollout restart ds kube-proxy
      kubectl -n kube-system rollout status ds kube-proxy
      kubectl -n kube-public get cm cluster-info -o yaml > /root/cluster-info.yaml
      sed -i 's/6443/8443/g' /root/cluster-info.yaml
      kubectl apply -f /root/cluster-info.yaml
  - name: Generate join command for control-plane
    shell: echo "$(kubeadm token create --print-join-command) --control-plane --certificate-key $(kubeadm init phase upload-certs --upload-certs | grep -vw -e certificate -e Namespace)"
    register: join_command_controller
  - name: Copy join command for control-plane to local file
    local_action: copy content="{{ join_command_controller.stdout_lines[0] }}" dest="./join-command-controller"
  - name: Generate join command for workers
    command: kubeadm token create --print-join-command
    register: join_command_workers
  - name: Copy join command for workers to local file
    local_action: copy content="{{ join_command_workers.stdout_lines[0] }}" dest="./join-command-workers"
- hosts: master002
  tasks:
  - name: Copy the join command to control plane
    copy:
      mode: 0777
      src: join-command-controller
      dest: /tmp/join-command.sh
  - name: Join the control plane node to cluster
    command: sh /tmp/join-command.sh
  - name: Copy kubeconfig
    shell: |
      mkdir -p /root/.kube
      cp -i /etc/kubernetes/admin.conf /root/.kube/config
      chown $(id -u):$(id -g) /root/.kube/config
- hosts: master003
  tasks:
  - name: Copy the join command to control plane
    copy:
      mode: 0777
      src: join-command-controller
      dest: /tmp/join-command.sh
  - name: Join the control plane node to cluster
    command: sh /tmp/join-command.sh
  - name: Copy kubeconfig
    shell: |
      mkdir -p /root/.kube
      cp -i /etc/kubernetes/admin.conf /root/.kube/config
      chown $(id -u):$(id -g) /root/.kube/config
- hosts: workers
  tasks:
  - name: Copy the join command to workers
    copy:
      mode: 0777
      src: join-command-workers
      dest: /tmp/join-command.sh
  - name: Join the workers node to cluster
    command: sh /tmp/join-command.sh
- hosts: master001
  tasks:
  - name: Labeling worker nodes
    shell: for hosts in $(kubectl get nodes | grep worker | awk '{print$1}'); do kubectl label nodes $hosts type=app; kubectl label nodes $hosts node-role.kubernetes.io/worker=; done
  - name: Confirm if you want to create metallb
    pause:
      prompt: "Do you want to create metallb? (yes/no)"
    register: create_metallb
  - name: Setup metallb
    shell: |
      kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.10/manifests/namespace.yaml
      kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.10/manifests/metallb.yaml
    when: create_metallb.user_input | bool
  - name: Waiting metallb controller ready
    pause:
      echo: yes
      minutes: 2
    when: create_metallb.user_input | bool
  - name: Apply metallb configuration
    shell: |
      wget https://raw.githubusercontent.com/metallb/metallb/v0.10/manifests/example-layer2-config.yaml -O /root/metallb-config.yaml
      sed -i 's+192.168.1.240/28+10.15.0.200-10.15.0.250+g' /root/metallb-config.yaml
      kubectl apply -f /root/metallb-config.yaml
    when: create_metallb.user_input | bool
  - name: Confirm if you want to create kubernetes dashboard
    pause:
      prompt: "Do you want to create kubernetes dashboard? (yes/no)"
    register: create_kubernetes_dashboard
  - name: Install kubernetes dashboard
    shell: kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.5.0/aio/deploy/recommended.yaml
    when: create_kubernetes_dashboard.user_input | bool
  - name: Copy k8s dashboard config file
    copy:
      src: '{{item}}'
      dest: /root/
    loop:
      - manifest/user-k8s-dashboard.yaml
      - scripts/kubeconfig-dashboard.sh
    when: create_kubernetes_dashboard.user_input | bool
  - name: Creating user and kubeconfig for kubernetes dashboard
    shell: |
      kubectl apply -f /root/user-k8s-dashboard.yaml
      bash /root/kubeconfig-dashboard.sh
    when: create_kubernetes_dashboard.user_input | bool
  - name: Confirm if you want to create loadbalancer for k8s dashboard
    pause:
      prompt: "Do you want to create loadbalancer for k8s dashboard? (yes/no)"
    register: create_lb_ingress_controller
  - name: Creating loadbalancer for k8s dashboard
    shell: |
      kubectl -n kubernetes-dashboard get svc kubernetes-dashboard -o yaml > /root/kubernetes-dashboard-svc.yaml
      sed -i 's+ClusterIP+LoadBalancer+g' /root/kubernetes-dashboard-svc.yaml
      kubectl apply -f /root/kubernetes-dashboard-svc.yaml
    when: create_lb_ingress_controller.user_input | bool
  - name: Confirm if you want to create nginx ingress controller
    pause:
      prompt: "Do you want to create nginx ingress controller? (yes/no)"
    register: create_ingress_controller
  - name: Install nginx ingress controller
    shell: kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/ingress-nginx-2.5.0/deploy/static/provider/baremetal/deploy.yaml
    when: create_ingress_controller.user_input | bool
  - name: Increasing ingress controller replica
    shell: kubectl -n ingress-nginx scale deployment ingress-nginx-controller --replicas=3
    when: create_ingress_controller.user_input | bool
  - name: Confirm if you want to create loadbalancer for nginx-ingress
    pause:
      prompt: "Do you want to create loadbalancer for nginx-ingress? (yes/no)"
    register: create_lb_ingress_controller
  - name: Creating loadbalancer for nginx-ingress
    shell: |
      kubectl -n ingress-nginx get svc ingress-nginx-controller -o yaml > /root/ingress-nginx-svc.yaml
      sed -i 's+NodePort+LoadBalancer+g' /root/ingress-nginx-svc.yaml
      kubectl apply -f /root/ingress-nginx-svc.yaml
    when: create_lb_ingress_controller.user_input | bool
  - name: Cleaning manifest
    file:
      path: "{{ item }}"
      state: absent
    with_items:
      - /root/cluster-info.yaml
      - /root/kube-proxy.yaml
      - /root/kubeadm-config.yaml
      - /root/metallb-config.yaml
      - /root/ingress-nginx-svc.yaml
      - /root/kubernetes-dashboard-svc.yaml
      - /root/user-k8s-dashboard.yaml
      - /root/kubeconfig-dashboard.sh
- hosts: localhost
  connection: local
  tasks:
  - name: Removing join command
    file:
      path: "{{ item }}"
      state: absent
    with_items:
      - join-command-controller
      - join-command-workers
